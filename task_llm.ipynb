{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Реализация и сравнение Reflex Attention с обычным трансформером\n",
        "\n",
        "## Введение\n",
        "\n",
        "Цель данного проекта — реализовать механизм Reflex Attention в модели трансформера (на основе NanoGPT) и сравнить его с обычным трансформером. Reflex Attention добавляет cross-attention к предыдущим слоям в каждом блоке трансформера, что позволяет модели получать доступ к ранним представлениям и улучшать качество работы.\n",
        "\n",
        "### Этапы работы\n",
        "\n",
        "1. Реализация Reflex Attention в NanoGPT.\n",
        "2. Обучение двух моделей: стандартного трансформера и трансформера с Reflex Attention.\n",
        "3. Сравнение производительности на датасете OpenWebText.\n",
        "4. Эксперименты с различными настройками и анализ результатов.\n",
        "\n",
        "## Часть 1: Реализация Reflex Attention\n",
        "\n",
        "### Что такое Reflex Attention?\n",
        "\n",
        "Reflex Attention изменяет стандартную архитектуру трансформера, добавляя cross-attention к предыдущим слоям. На $i$-м слое внимание вычисляется следующим образом:\n",
        "\n",
        "$$\n",
        "\\text{Attn}_i = \\text{Concat}\\left[\\text{SA}(h_i),\\ \\text{CA}(h_{i-1}, h_i),\\ \\text{CA}(h_{i-2}, h_i)\\right],\n",
        "$$\n",
        "\n",
        "где:\n",
        "\n",
        "- **SA** (Self-Attention): стандартное внимание на текущих представлениях $h_i$.\n",
        "- **CA** (Cross-Attention): внимание, где:\n",
        "  - запросы ($Q$) берутся из $h_i$,\n",
        "  - ключи и значения ($K$, $V$) берутся из предыдущих слоев: $h_{i-1}$ и $h_{i-2}$.\n",
        "\n",
        "### Модификация модели\n",
        "\n",
        "#### 1. Клонирование репозитория NanoGPT\n",
        "\n",
        "Клонируем репозиторий NanoGPT:\n",
        "\n"
      ],
      "metadata": {
        "id": "MBVUSybVEJdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/karpathy/nanoGPT.git\n",
        "!cd nanoGPT"
      ],
      "metadata": {
        "id": "5oGkN0inEJzX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8df0f1d-5aaf-454a-f5a2-4e511d0a0c59"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 682, done.\u001b[K\n",
            "remote: Total 682 (delta 0), reused 0 (delta 0), pack-reused 682 (from 1)\u001b[K\n",
            "Receiving objects: 100% (682/682), 952.47 KiB | 23.81 MiB/s, done.\n",
            "Resolving deltas: 100% (385/385), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Изменения в model.py\n",
        "Модифицируем файл model.py для реализации Reflex Attention.\n",
        "\n",
        "a. Изменение класса Block\n",
        "Добавляем в блок трансформера поддержку Reflex Attention:"
      ],
      "metadata": {
        "id": "BXY-8SPr-M2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Трансформер-блок с Reflex Attention\"\"\"\n",
        "\n",
        "    def __init__(self, config, layer_idx):\n",
        "        super().__init__()\n",
        "        self.layer_idx = layer_idx\n",
        "        self.n_head = config.n_head\n",
        "\n",
        "        # Разделение числа голов между SA и CA\n",
        "        self.n_head_sa = 2  # Голов для Self-Attention\n",
        "        self.n_head_ca1 = 2  # Голов для Cross-Attention с h_{i-1}\n",
        "        self.n_head_ca2 = 2  # Голов для Cross-Attention с h_{i-2}\n",
        "\n",
        "        # Проверка на корректность количества голов\n",
        "        assert self.n_head_sa + self.n_head_ca1 + self.n_head_ca2 == self.n_head, \"Сумма голов должна быть равна общему количеству голов\"\n",
        "\n",
        "        # Self-Attention\n",
        "        self.sa = CausalSelfAttention(config, n_head=self.n_head_sa)\n",
        "\n",
        "        # Cross-Attention с предыдущими слоями\n",
        "        if self.layer_idx >= 1:\n",
        "            self.ca1 = CrossAttention(config, n_head=self.n_head_ca1)\n",
        "        else:\n",
        "            self.ca1 = None\n",
        "\n",
        "        if self.layer_idx >= 2:\n",
        "            self.ca2 = CrossAttention(config, n_head=self.n_head_ca2)\n",
        "        else:\n",
        "            self.ca2 = None\n",
        "\n",
        "        # Нормализация и MLP\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            nn.Dropout(config.dropout),\n",
        "        )"
      ],
      "metadata": {
        "id": "4FhGV2E4-Nz3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Реализация Cross-Attention"
      ],
      "metadata": {
        "id": "p1JobDOZ-Pi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    \"\"\"Cross-Attention модуль\"\"\"\n",
        "\n",
        "    def __init__(self, config, n_head):\n",
        "        super().__init__()\n",
        "        self.n_head = n_head\n",
        "        self.head_dim = config.n_embd // config.n_head\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        self.q_proj = nn.Linear(config.n_embd, n_head * self.head_dim)\n",
        "        self.k_proj = nn.Linear(config.n_embd, n_head * self.head_dim)\n",
        "        self.v_proj = nn.Linear(config.n_embd, n_head * self.head_dim)\n",
        "        self.out_proj = nn.Linear(n_head * self.head_dim, config.n_embd)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x, k_v, mask=None):\n",
        "        B, T, _ = x.size()\n",
        "\n",
        "        # Проекция запросов, ключей и значений\n",
        "        q = self.q_proj(x).view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
        "        k = self.k_proj(k_v).view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
        "        v = self.v_proj(k_v).view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Вычисление внимания\n",
        "        attn_weights = (q @ k.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            attn_weights = attn_weights.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attn_probs = F.softmax(attn_weights, dim=-1)\n",
        "        attn_probs = self.dropout(attn_probs)\n",
        "\n",
        "        y = attn_probs @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, -1)\n",
        "        y = self.out_proj(y)\n",
        "        return y\n",
        "\n"
      ],
      "metadata": {
        "id": "5et1UkKp-RwX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c. Обновление метода forward в Block"
      ],
      "metadata": {
        "id": "EEHSmgQ0-Trf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, x, h_prev_layers):\n",
        "    \"\"\"\n",
        "    x: Текущее скрытое состояние [B, T, C]\n",
        "    h_prev_layers: Список предыдущих скрытых состояний [h_{i-1}, h_{i-2}]\n",
        "    \"\"\"\n",
        "    # Нормализация входа\n",
        "    x_norm = self.ln1(x)\n",
        "\n",
        "    # Self-Attention\n",
        "    y_sa = self.sa(x_norm)\n",
        "\n",
        "    attentions = [y_sa]\n",
        "\n",
        "    # Cross-Attention с h_{i-1}\n",
        "    if self.ca1 is not None and len(h_prev_layers) >= 1:\n",
        "        h_prev1_norm = self.ln1(h_prev_layers[-1])\n",
        "        y_ca1 = self.ca1(x_norm, h_prev1_norm)\n",
        "        attentions.append(y_ca1)\n",
        "\n",
        "    # Cross-Attention с h_{i-2}\n",
        "    if self.ca2 is not None and len(h_prev_layers) >= 2:\n",
        "        h_prev2_norm = self.ln1(h_prev_layers[-2])\n",
        "        y_ca2 = self.ca2(x_norm, h_prev2_norm)\n",
        "        attentions.append(y_ca2)\n",
        "\n",
        "    # Объединение результатов внимания\n",
        "    y = torch.cat(attentions, dim=-1)\n",
        "    x = x + y\n",
        "\n",
        "    # Пропуск через MLP\n",
        "    x = x + self.mlp(self.ln2(x))\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "udpwJsU6-UYv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.transformer = nn.ModuleDict({\n",
        "            'wte': nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            'wpe': nn.Embedding(config.block_size, config.n_embd),\n",
        "            'drop': nn.Dropout(config.dropout),\n",
        "            'h': nn.ModuleList([Block(config, layer_idx=i) for i in range(config.n_layer)]),\n",
        "            'ln_f': nn.LayerNorm(config.n_embd),\n",
        "        })\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, \"Длина последовательности превышает block_size\"\n",
        "\n",
        "        # Получение эмбеддингов\n",
        "        token_embeddings = self.transformer.wte(idx)  # [b, t, n_embd]\n",
        "        position_embeddings = self.transformer.wpe(torch.arange(t, device=device))  # [t, n_embd]\n",
        "        x = self.transformer.drop(token_embeddings + position_embeddings)\n",
        "\n",
        "        # Список скрытых состояний для Reflex Attention\n",
        "        h_prev_layers = []\n",
        "\n",
        "        # Проход через блоки\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x, h_prev_layers)\n",
        "            h_prev_layers.append(x)\n",
        "\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "            return logits, loss\n",
        "        else:\n",
        "            return logits\n"
      ],
      "metadata": {
        "id": "kDP3JT07YxbG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Часть 2: Обучение и эксперименты\n",
        "\n",
        "## Подготовка данных\n",
        "\n",
        "Используем датасет OpenWebText."
      ],
      "metadata": {
        "id": "Z1vBkC5B-ewP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python data/openwebtext/prepare.py"
      ],
      "metadata": {
        "id": "lbANtJiQ-1qo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d44d586-d249-431a-ed45-920f033eb16c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/data/openwebtext/prepare.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls nanoGPT\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyWmwjEycdBK",
        "outputId": "19338aee-fb89-4445-e51d-6ce6deb4ea2e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assets\t  config\t   data     model.py   sample.py\t   train.py\n",
            "bench.py  configurator.py  LICENSE  README.md  scaling_laws.ipynb  transformer_sizing.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Настройка конфигураций\n",
        "Создаем две конфигурации: для стандартного трансформера и трансформера с Reflex Attention.\n",
        "\n",
        "a. Конфигурация стандартного трансформера"
      ],
      "metadata": {
        "id": "nb7mpDqu-2af"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nanoGPT/config/train_standard.py\n",
        "\n",
        "# Базовые параметры (ранее из train_shakespeare.py)\n",
        "n_layer = 4  # Количество слоев в модели\n",
        "n_head = 4   # Количество голов в self-attention\n",
        "n_embd = 256  # Размерность эмбеддингов\n",
        "batch_size = 32  # Размер батча\n",
        "learning_rate = 3e-4  # Начальная скорость обучения\n",
        "max_iters = 1000  # Максимальное количество итераций\n",
        "\n",
        "# Переопределяем параметры для текущего эксперимента\n",
        "out_dir = 'out-standard'\n",
        "eval_interval = 500  # Интервал оценки модели\n",
        "eval_iters = 200      # Число итераций для оценки\n",
        "log_interval = 100    # Интервал логирования\n",
        "\n",
        "# Гиперпараметры модели\n",
        "n_layer = 6  # Изменяем количество слоев\n",
        "n_head = 6   # Изменяем количество голов\n",
        "n_embd = 384  # Изменяем размерность эмбеддингов\n",
        "\n",
        "# Гиперпараметры обучения\n",
        "batch_size = 64  # Увеличиваем размер батча\n",
        "learning_rate = 3e-4  # Оставляем скорость обучения без изменений\n",
        "max_iters = 5000  # Увеличиваем количество итераций\n",
        "\n"
      ],
      "metadata": {
        "id": "ebFGgnVj-3pA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Конфигурация Reflex Attention"
      ],
      "metadata": {
        "id": "GRW8Px0b-49n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nanoGPT/config/train_reflex.py\n",
        "\n",
        "# Базовые параметры\n",
        "out_dir = 'out-reflex'          # Директория для сохранения результатов\n",
        "eval_interval = 500             # Интервал оценки\n",
        "eval_iters = 200                # Число итераций для оценки\n",
        "log_interval = 100              # Интервал логирования\n",
        "\n",
        "# Гиперпараметры модели\n",
        "n_layer = 6                     # Количество слоев трансформера\n",
        "n_head = 6                      # Количество голов внимания\n",
        "n_embd = 384                    # Размерность эмбеддингов\n",
        "\n",
        "# Гиперпараметры обучения\n",
        "batch_size = 32                 # Размер батча\n",
        "learning_rate = 3e-4            # Скорость обучения\n",
        "max_iters = 50000               # Максимальное количество итераций\n",
        "\n"
      ],
      "metadata": {
        "id": "0Q-ud2GE-6FX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запуск обучения\n",
        "1. Стандартный трансформер"
      ],
      "metadata": {
        "id": "2nzivn3N-8l2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/train_standard.py"
      ],
      "metadata": {
        "id": "sz--XGUq-9XH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fc1f69b-0037-4443-e32f-0e101ce09c43"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/train.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Reflex Attention"
      ],
      "metadata": {
        "id": "hGNlFbcj--lX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/train_reflex.py"
      ],
      "metadata": {
        "id": "Ot-Yv6iL_ANf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25df91d4-9c92-4903-c5a5-fe1b003707ab"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/train.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tensorboard --logdir out-standard --port 6006\n",
        "!tensorboard --logdir out-reflex --port 6007"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9TDVPfFY8ZN",
        "outputId": "7965230c-96a2-451a-d71f-4519d896e541"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-27 07:12:29.605576: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-27 07:12:29.630743: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-27 07:12:29.638233: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-27 07:12:29.656544: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-27 07:12:31.026112: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "NOTE: Using experimental fast data loading logic. To disable, pass\n",
            "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
            "    https://github.com/tensorflow/tensorboard/issues/4784\n",
            "\n",
            "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
            "TensorBoard 2.17.1 at http://localhost:6006/ (Press CTRL+C to quit)\n",
            "2024-11-27 07:13:08.975674: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-27 07:13:09.002403: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-27 07:13:09.010170: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-27 07:13:09.027204: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-27 07:13:10.372970: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "NOTE: Using experimental fast data loading logic. To disable, pass\n",
            "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
            "    https://github.com/tensorflow/tensorboard/issues/4784\n",
            "\n",
            "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
            "TensorBoard 2.17.1 at http://localhost:6007/ (Press CTRL+C to quit)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Часть 3: Результаты экспериментов\n",
        "Наблюдения\n",
        "После завершения обучения ожидаем следующие результаты:\n",
        "\n",
        "Скорость сходимости:\n",
        "\n",
        "Reflex Attention может быстрее сходиться за счет более богатых представлений.\n",
        "Однако это может потребовать большего времени на вычисления.\n",
        "Качество:\n",
        "\n",
        "Reflex Attention должен показать лучшие результаты на длинных последовательностях, где важно учитывать ранний контекст.\n",
        "Возможные проблемы\n",
        "Увеличение памяти: Reflex Attention требует хранения состояний предыдущих слоев.\n",
        "Переобучение: Из-за увеличения параметров модель может переобучиться.\n",
        "Часть 4: Дополнительные эксперименты\n",
        "Эксперимент 1: Количество голов\n",
        "Цель: Проверить, как распределение голов между SA и CA влияет на результаты.\n",
        "Настройки:\n",
        "Больше голов для SA, меньше для CA.\n",
        "Равное количество голов для SA и CA.\n",
        "Эксперимент 2: Влияние на разные слои\n",
        "Цель: Изучить, как Reflex Attention на разных слоях влияет на обучение.\n",
        "Настройки:\n",
        "Применение Reflex Attention только на верхних слоях.\n",
        "Исключение Reflex Attention из нижних слоев.\n",
        "Выводы\n",
        "Что получилось:\n",
        "\n",
        "Реализован Reflex Attention с разделением внимания на текущий и предыдущие слои.\n",
        "Подготовлены эксперименты для сравнения с базовым трансформером.\n",
        "Что может быть улучшено:\n",
        "\n",
        "Оптимизация памяти и вычислений.\n",
        "Эксперименты с более сложными конфигурациями.\n"
      ],
      "metadata": {
        "id": "wE-qtr9H_A-3"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}